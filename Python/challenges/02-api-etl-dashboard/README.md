# ğŸ Python â€” The Data Magician  
### Challenge 02: API â†’ ETL â†’ Mini Dashboard

---

## ğŸ§© Context

Modern backends and data workflows live on APIs:
- You fetch data from an external service,
- Clean and transform it,
- Store it,
- And expose some kind of report.

In this challenge, you'll build a **tiny ETL pipeline in Python**:
API â†’ process â†’ store â†’ generate a simple HTML/Markdown dashboard.

---

## ğŸ¯ Objective

Create a Python script (or small package) that:

1. Fetches data from a **public API** (e.g. countries, crypto prices, PokÃ©mon, GitHub repos â€” your choice).
2. Normalizes and filters the data.
3. Stores the result locally:
   - as a `.json` or `.csv` file.
4. Generates a **static report**:
   - in `.md` or `.html`,
   - with top N entries based on some metric (population, stars, price, etc.),
   - and summary stats.

Example idea:  
â€œFetch top starred Python repos from GitHub and generate a report of the top 20 with stars, URL, and description.â€

---

## âš™ï¸ Technical Requirements

- [ ] Use only standard library **plus** `requests` (or `urllib` if you want 100% stdlib).
- [ ] Structure:
  - `fetch_data()` â†’ calls API.
  - `transform_data()` â†’ cleans & filters.
  - `save_data()` â†’ writes JSON/CSV.
  - `generate_report()` â†’ creates `.md`/`.html`.
  - `main()` â†’ orchestrates.
- [ ] Handle:
  - Network errors (timeouts, bad responses).
  - Empty or malformed responses.
- [ ] Allow configuration:
  - via CLI args (e.g. `--limit`, `--output-format`, `--min-stars`),
  - or via a small config dict in code.

---

## ğŸš« Rules

- No full web frameworks (Flask, Django) for this challenge.
- No heavy data libs (`pandas`, `numpy`) in the base version.
- Do not hard-code secrets or tokens in the code (if API requires one, read from env var).
